<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en" dir="ltr">
<head>
<title>454 Bioinformatics Pipeline - BayPaulWiki</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="generator" content="MediaWiki 1.16.5" />
<link rel="shortcut icon" href="/favicon.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="/orderforms/wiki/opensearch_desc.php" title="BayPaulWiki (en)" />
<link rel="alternate" type="application/atom+xml" title="BayPaulWiki Atom feed" href="/orderforms/wiki/index.php?title=Special:RecentChanges&amp;feed=atom" />
<link rel="stylesheet" href="/orderforms/wiki/skins/common/shared.css?270" media="screen" />
<link rel="stylesheet" href="/orderforms/wiki/skins/common/commonPrint.css?270" media="print" />
<link rel="stylesheet" href="/orderforms/wiki/skins/monobook/main.css?270" media="screen" />
<!--[if lt IE 5.5000]><link rel="stylesheet" href="/orderforms/wiki/skins/monobook/IE50Fixes.css?270" media="screen" /><![endif]-->
<!--[if IE 5.5000]><link rel="stylesheet" href="/orderforms/wiki/skins/monobook/IE55Fixes.css?270" media="screen" /><![endif]-->
<!--[if IE 6]><link rel="stylesheet" href="/orderforms/wiki/skins/monobook/IE60Fixes.css?270" media="screen" /><![endif]-->
<!--[if IE 7]><link rel="stylesheet" href="/orderforms/wiki/skins/monobook/IE70Fixes.css?270" media="screen" /><![endif]-->
<link rel="stylesheet" href="/orderforms/wiki/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=18000&amp;action=raw&amp;maxage=18000" />
<link rel="stylesheet" href="/orderforms/wiki/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=18000&amp;action=raw&amp;maxage=18000" media="print" />
<link rel="stylesheet" href="/orderforms/wiki/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=18000&amp;action=raw&amp;maxage=18000" />
<link rel="stylesheet" href="/orderforms/wiki/index.php?title=-&amp;action=raw&amp;maxage=18000&amp;gen=css" />
<script>
var skin="monobook",
stylepath="/orderforms/wiki/skins",
wgUrlProtocols="http\\:\\/\\/|https\\:\\/\\/|ftp\\:\\/\\/|irc\\:\\/\\/|gopher\\:\\/\\/|telnet\\:\\/\\/|nntp\\:\\/\\/|worldwind\\:\\/\\/|mailto\\:|news\\:|svn\\:\\/\\/",
wgArticlePath="/orderforms/wiki/index.php?title=$1",
wgScriptPath="/orderforms/wiki",
wgScriptExtension=".php",
wgScript="/orderforms/wiki/index.php",
wgVariantArticlePath=false,
wgActionPaths={},
wgServer="https://jbpc.mbl.edu",
wgCanonicalNamespace="",
wgCanonicalSpecialPageName=false,
wgNamespaceNumber=0,
wgPageName="454_Bioinformatics_Pipeline",
wgTitle="454 Bioinformatics Pipeline",
wgAction="view",
wgArticleId=1434,
wgIsArticle=true,
wgUserName=null,
wgUserGroups=null,
wgUserLanguage="en",
wgContentLanguage="en",
wgBreakFrames=false,
wgCurRevisionId=7378,
wgVersion="1.16.5",
wgEnableAPI=true,
wgEnableWriteAPI=true,
wgSeparatorTransformTable=["", ""],
wgDigitTransformTable=["", ""],
wgMainPageTitle="Main Page",
wgFormattedNamespaces={"-2": "Media", "-1": "Special", "0": "", "1": "Talk", "2": "User", "3": "User talk", "4": "BayPaulWiki", "5": "BayPaulWiki talk", "6": "File", "7": "File talk", "8": "MediaWiki", "9": "MediaWiki talk", "10": "Template", "11": "Template talk", "12": "Help", "13": "Help talk", "14": "Category", "15": "Category talk"},
wgNamespaceIds={"media": -2, "special": -1, "": 0, "talk": 1, "user": 2, "user_talk": 3, "baypaulwiki": 4, "baypaulwiki_talk": 5, "file": 6, "file_talk": 7, "mediawiki": 8, "mediawiki_talk": 9, "template": 10, "template_talk": 11, "help": 12, "help_talk": 13, "category": 14, "category_talk": 15, "image": 6, "image_talk": 7},
wgSiteName="BayPaulWiki",
wgCategories=[],
wgRestrictionEdit=[],
wgRestrictionMove=[];
</script><script src="/orderforms/wiki/skins/common/wikibits.js?270"></script>
<script src="/orderforms/wiki/skins/common/ajax.js?270"></script>
<script src="/orderforms/wiki/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook&amp;270"></script>

</head>
<body class="mediawiki ltr ns-0 ns-subject page-454_Bioinformatics_Pipeline skin-monobook">
<div id="globalWrapper">
<div id="column-content"><div id="content" >
	<a id="top"></a>
	
	<h1 id="firstHeading" class="firstHeading">454 Bioinformatics Pipeline</h1>
	<div id="bodyContent">
		<h3 id="siteSub">From BayPaulWiki</h3>
		<div id="contentSub"></div>
		<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>
		<!-- start content -->
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Before_you_begin.21"><span class="tocnumber">1</span> <span class="toctext">Before you begin!</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Move_the_454_data_files_to_shire_and_backup_disk"><span class="tocnumber">2</span> <span class="toctext">Move the 454 data files to shire and backup disk</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Import_Info_and_Sequences_into_env454"><span class="tocnumber">3</span> <span class="toctext">Import Info and Sequences into env454</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Trim_the_sequences"><span class="tocnumber">4</span> <span class="toctext">Trim the sequences</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#GAST_-_distance_to_nearest_RefHVR_references"><span class="tocnumber">5</span> <span class="toctext">GAST - distance to nearest RefHVR references</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Uploading_sequences_to_VAMPS"><span class="tocnumber">6</span> <span class="toctext">Uploading sequences to VAMPS</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Clustering_Sequences"><span class="tocnumber">7</span> <span class="toctext">Clustering Sequences</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Adding_new_users"><span class="tocnumber">8</span> <span class="toctext">Adding new users</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Taxonomy_Spreadsheets"><span class="tocnumber">9</span> <span class="toctext">Taxonomy Spreadsheets</span></a></li>
</ul>
</td></tr></table><script>if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } </script>
<h2> <span class="mw-headline" id="Before_you_begin.21"> Before you begin! </span></h2>
<p>It is standard procedure to include a run_* script for all of the scripts discussed below.  This allows staff to review the files and script parameters that were used.  By reviewing previous pipeline directories (such as sff_* and VERSION3 in the g454 project directory) you can view and copy/edit existing run_* scripts.  New users require a ".dbconf" and a ".my.cnf" in order to process runs through the pipeline.
</p>
<h2> <span class="mw-headline" id="Move_the_454_data_files_to_shire_and_backup_disk"> Move the 454 data files to shire and backup disk </span></h2>
<p>Bring the sff.txt files into the database where they can be processed and analyzed.
</p><p>New on the FLX:
1.  Backup the Run
There is a text file on the Desktop called FLX backup instructions:
ssh into FLX 10.1.0.14 to get name of dir to be backed up
</p>
<pre>    ssh adminrig@10.1.0.14
    log in and cd to data, find dir name, and logout
</pre>
<p>move into the "Data to Archive" dir 
</p>
<pre>   cd /Volumes/Data\To\Archive/
</pre>
<p>secure copy the remote files
</p>
<pre>   scp -r adminrig@10.1.0.14:/data/rundir .
</pre>
<p>2.Copy the files from "Data to Archive" to the run dir in g454/downloads
</p>
<pre>  Create a dir named the rundate (YYYYMMDD) in g454/downloads
    cd to the run dir in "Data to Archive" the D_dir
    scp *.* cholmes@arthur.mbl.edu:~/g454/downloads/run dir
    cd to the sff dir
    scp *.sff cholmes@arthur.mbl.edu:~/g454/downloads/run dir
</pre>
<h2> <span class="mw-headline" id="Import_Info_and_Sequences_into_env454">Import Info and Sequences into env454</span></h2>
<p>1.  Log on to one of the bioinformatics machines (arthur).
</p><p>2.  Review the Sequencing Request form.  This file always contains "SeqReq" in the file name with a .csv extension.  It can be viewed with the simple "cat" or "less" command.  If the researcher entered 'none' for primer, remove this and leave the cells blank.
</p><p>3.  Check the permissions on the .csv file.  If necessary change the group and permissions.  To view the permissions on all files in your directory use the command "ls -l"
</p>
<pre>    If the permissions look like this:
    -rw-rw-r-- 1 cholmes cholmes 969 Oct 19 13:36 GSFLXSeqReq_20071018_DMW.csv
    
    they need to be changed to look like this:
    -rw-rw-r-- 1 cholmes g454 969 Oct 19 13:36 GSFLXSeqReq_20071018_DMW.csv
    
    Change them with the following commands:
         a.  chgrp g454 GSFLXSeqReq_20071018_DMW.csv
         b.  chmod 664 GSFLXSeqReq_20071018_DMW.csv
</pre>
<pre>  Convert the file to UNIX with mac2unix or dos2unix commands.
     mac2unix GSFLXSeqReq_20071018_DMW.csv
     dos2unix GSFLXSeqReq_20071018_DMW.csv
</pre>
<p>NOTE: If you cannot distinguish between dos and unix formatted files, use the dos2unix command anyway to convert the files.  If you import the file into BBedit or Text Wrangler you can see how the file is formatted. Alternatively, you can use the unix command cat -v to tell if a file is MAC versus DOS or UNIX formatted. 
</p><p>4.  Check the csv files for having a project description, proper dataset names, or any columns missing data.  "less GSFLXSeqReq_20071018_DMW.csv".  Also check for duplicate run keys.  If there are duplicates, they should be in different regions with different primers.   The csv file is ordered alphabetically by run key (Processing starts here). Hilary has written a script that will check for duplicated keys called "dupkeys".  It should be run from your run directory.
</p><p><br />
5. Check the primers.  They should be in the following list (not case sensitive):
</p>
<pre>        Bacterial V6-V4 Suite
        Bacterial V3-V5 Suite
        Archaeal V6-V4 Suite
        Eukaryal V9 Suite
        Archaeal V6 Suite
        Bacterial V6 Suite
        bacv6
        arcv6
        eukv9
        topo
        Relman
        eukv9_1389
        eukv9_1380
        v3
        v6_dutch
</pre>
<p>5b. Check the sources.  They should be in the following list of current, commonly used Ti amplicon sources:
</p><p><br />
</p>
<pre>        v6v4  (bacterial)
        v3v5  (bacterial)
        v6v4a (archaeal)
        v9    (eukaryal)
        v6    (bacterial)
        v6a   (archaeal)
</pre>
<p><br />
</p><p>6.  Import the project, researcher, run keys, and dataset information with import_flxruninfo.  If samples are external, double-check the run date to see if it already exists in run_keys.  This will write into tables: projects, run_keys, run_primers
</p>
<pre>    import_flxruninfo -r rundate -f SeqReq_MBJ_GOS_Bv3v5.csv
</pre>
<p>7.  Import the sff files into the database with import_flxrun, import once for each region.  This will write into rawseq, rawqual, rawflow, rawflowindex.  The ampersand "&amp;" allows the command to run in the background.
</p>
<pre>    import_flxrun -r rundate -i EFWL25X01.sff &amp;
    import_flxrun -r rundate -i EFWL25X02.sff &amp;
</pre>
<p>8.  Check that the sequences imported fully.  Often the database connection is dropped and the import fails before all sequences are imported.
</p>
<pre>       a.  determine the number of reads you should have:
             head sff/*.sff.txt | grep "# of Reads:" 
  
       b.  determine the number of reads that made it into the database:
       mysql -h jbpcdb.mbl.edu env454 -e "SELECT region, count(*) as seqcnt FROM rawseq WHERE run=20081125 group by region order by region"
 
</pre>
<p><br />
</p>
<pre>       *** If the counts for each region do not match the number of reads in each sff file, the data will need to be cleared out and rerun
             where rundate is the actual date of the run, and # is 1 or 2 (which region to clear out)
            
             clear454run -raw -reg # -run rundate
   
            and, import that region again (import_flxrun...)
</pre>
<pre>       c.  If the counts are the same, then run the statistics on the reads (in the downloads/rundate directory) 
            report_trimmingstats -raw rundate
</pre>
<pre>            Review the results in trimstats_rundate.txt.  Be sure that counts are correct and that all datasets have reasonable numbers of reads
            (at least several thousand) and that runkeys were not found in the wrong region in numbers &gt; 100.
  
       
</pre>
<pre>      The report above uses the following queries: 
       If the sameregion = 0, and expectedregion is not 0 and there numerous reads (&gt;40) something may be wrong
     
       If the value of the sameregion field is 1, and there are few reads, that is also suspect.
     
       select r.region as foundregion, count(r.read_id) as seqcnt, k.run_key as run_key, 
       k.region as expectedregion, dataset, (r.region = k.region) as sameregion
       from (select substr(sequence, 1, 5) as run_key, region, read_id from rawseq where run='20070802') as r 
       left join run_keys as k 
       using (run_key)
       where k.run='20070802'
       group by r.run_key, r.region
       order by seqcnt desc 
       
       Same query as above ordered by region and dataset:
       
       select  k.run_key as run_key,dataset,   k.region as expectedregion, count(r.read_id) as seqcnt,
       r.region as foundregion, (r.region = k.region) as same_region
       from (select substr(sequence, 1, 5) as run_key, region, read_id from rawseq where run='20081030') as r 
       left join run_keys as k 
       using (run_key)
       where k.run='20081030'
       group by r.run_key, r.region
       order by expectedregion, foundregion, dataset
       
       If the value of seqcnt is low for any dataset, it is suspect
     
       For Run Keys in only one region (region&nbsp;!= 0)
       select k.run_key, k.region, dataset, count(r.read_id) as seqcnt
       from run_keys as k left join
       (select read_id, region, substr(sequence,1,5) as run_key from rawseq where run='20070802') as r
       using (run_key, region)
       where k.run='20070802' and k.region&nbsp;!=0
       group by k.run_key, k.region
       
       ALTERNATIVE:  If the Run Key is in both regions (region=0) use:
       select k.run_key,  r.region, dataset, count(r.read_id) as seqcnt
       from run_keys as k left join
       (select read_id, region, substr(sequence,1,5) as run_key from rawseq where run='20070823') as r
       using (run_key)
       where k.run='20070823' and k.region=0
       group by k.run_key, r.region
</pre>
<h2> <span class="mw-headline" id="Trim_the_sequences"> Trim the sequences </span></h2>
<p>Removes low quality reads and the primers from the high quality reads, and assigns dataset and project names to each read
</p>
<pre> 1.  Log on to the cluster (grendel).
      
       From the run directory submit clustertrim to the cluster and run:
       noclustertrim -r rundate
       
      You can check for when your processes have completed on the cluster:
       qstat
    
   or&nbsp;:
      qstat | grep your_username
  
</pre>
<pre> 2.  When the trimming is complete, finishing running the statistics (still in the downloads/run directory)
      
       report_trimmingstats -trim rundate
       
       This will run the following queries: 
        
       count the total reads
       SELECT count(*) from trimseq where run=20081120
    
        count reads for each dataset
         SELECT project, dataset, deleted, count(*) 
         FROM trimseq where run=20081120 and dataset&nbsp;!= <i> and project&nbsp;!= </i>
         GROUP BY dataset, deleted ORDER BY dataset, deleted
 
       count the reads that were deleted and why
       SELECT delete_reason, COUNT(*) 
       FROM trimseq WHERE run='20081120' 
       GROUP by delete_reason
     
       delete_reason=0 (not deleted) should be much greater than all other reasons.
       If any delete_reason is greater than 1% (except proximal), the trimming should 
       be investigated before proceeding. 
      
       If the total is greater than 15&nbsp;% look at the biggest reason.
      
       If  biggest delete_reason is 'key' then the regions may be confused try:
      
       SELECT substring(s.sequence, 1,5) as firstfive, region, count(t.read_id) as cnt, 
       COUNT(CASE WHEN deleted=0 then t.read_id ELSE NULL END) as cntkept, 
       COUNT(CASE WHEN deleted=1 then t.read_id ELSE NULL END) as cntdeleted 
       FROM `trimseq` as t join rawseq as s using (read_id) 
       WHERE t.run='20070514' and s.run='20070514' 
       GROUP BY region, firstfive 
       ORDER BY cnt desc
      
       This query shows the results for all keys.  Useful for finding lost keys, i.e. if no data 
       was recovered for a particular key:

       SELECT substr(sequence,1,5) as rkey, r.region, dataset, count(*) as cnt
       FROM rawseq as r left join run_keys as k on (r.run = k.run and r.region=k.region 
       and substr(sequence,1,5)=k.run_key)
       WHERE r.run='20080507'
       GROUP by rkey, r.region
       ORDER by cnt desc
</pre>
<h2> <span class="mw-headline" id="GAST_-_distance_to_nearest_RefHVR_references"> GAST - distance to nearest RefHVR references </span></h2>
<p>GAST (global alignment search tool) uses USearch to find a set of top hits for each sequence (unique read) in a reference database of sequences with accepted taxonomy (our RefHVR database), then does a global alignment and distance calculation to find the reference sequence(s) most similar to each 454 sequence.
GAST needs to be run separately for each hypervariable target region in the run.  For example, if you have both v6 and v9 data, 
run once for v6, the repeat it for v9.
</p><p>1. Log onto the cluster.
</p><p>2. From the gast directory (cd /xraid2-2/g454/gast), run clustergast for each hypervariable region sequenced, <i>i.e.</i>:
</p>
<pre>      clustergast -r 20100929 -s v6v4
      clustergast -r 20100929 -s v3v5
      clustergast -r 20100929 -s v6v4a
</pre>
<dl><dd> This script creates a directory (<i>e.g.</i> 20100929_v6v4) with a fasta file of all reads that were not deleted for quality and a second fasta file containing only the unique sequences represented by the reads, with a .names file that links each sequence to the reads it represents. It then splits up the file of sequences and runs the GAST algorithm in parallel, generating many .txt files as output.
</dd></dl>
<p>3. When the cluster processes have completed, log on to a bioware machine and confirm that all the sequences were upload by comparing the gast output in the *txt files with the contents of the new table in env454:
</p>
<dl><dd>Using mysql check the number of reads that were loaded:
</dd></dl>
<pre>     mysql -h jbpcdb.mbl.edu env454 -e "SELECT COUNT(distinct read_id) FROM gast_20100929_v6v4" 
</pre>
<dl><dd>Compare the result with the number of gast results in the g454/gast directory for each source you have used:
</dd></dl>
<pre>     cat /xraid2-2/g454/gast/20100929_v6v4/gast_20100929_v6v4.*.txt | cut -f1 | sort | uniq | wc -l
      
</pre>
<dl><dd>If this does not equal the number of reads in the gast_run table you will need to manually load the remaining data:
</dd></dl>
<pre>     cd $g454/gast/20100929_v6v4
     for i in gast_20100929_v6v4.*.txt; do mysqlimport -i -C -v -L -h jbpcdb env454 $i; done;       
     Check the count in the gast_20100929_v6v4 table again.  It should now match the results in the directory.
</pre>
<p>4.  Check to see how many sequences were assigned a taxonomy by GAST.  
The number of records in the gast table should be nearly equal to the number of sequences in the fasta file of unique sequences in the gast working directory.    
</p>
<pre>     Example:
     facount /xraid2-2/g454/gast/20100929_v6v4/20100929_v6v4.unique.fa  = 481,034 
     mysql -h jbpcdb.mbl.edu env454 -e "SELECT COUNT(distinct read_id) FROM gast_20100929_v6v4" = 471,803
     481,034 / 471,803 ~= 1.02, or 2% larger.  
          
</pre>
<dl><dd><dl><dd>The difference is the number of unique sequences &gt;=70% different from any sequence in the reference database.
</dd><dd>For bacterial samples, you may drop more or fewer sequences depending on the degree of eukaryotic DNA in your sample.
</dd><dd>You will likely drop more reads for eukaryotic and archaeal projects than for bacterial ones.    
</dd></dl>
</dd></dl>
<dl><dd><dl><dd>A very large difference indicates something went wrong in assigning taxonomy and you should confirm that your primers are correct and that you really amplified the same region that you told the pipeline you did.
</dd></dl>
</dd></dl>
<dl><dd>Note that the gast count will change after doing gast_cleanup below, so this count is only valid at this point in the process.  
</dd><dd>If the counts are off, first check that the initial set of sequences is correct
</dd></dl>
<pre>    grep -c "&gt;" 20100929_v6v4/*.fa
    mysql -h jbpcdb.mbl.edu env454 -e "SELECT COUNT(DISTINCT sequence) FROM trimseq WHERE deleted=0 AND run=20100929 AND source = 'v6v4'"
</pre>
<dl><dd>If anything goes wrong, the process can be restarted, see clustergast usage by typing 'clustergast' on the commandline.
<dl><dd>(Note: you can clear out the table first by: delete from gast_20100929_v6v4; [HGM])          
</dd></dl>
</dd></dl>
<pre>    Example of a restart: clustergast -r 20100929 -s v6v4 -start bestalign.
</pre>
<p>5.  Now move the data into the central gast tables:
</p>
<pre>       cd $g454/gast/20100929_v6v4
       gast_cleanup -r 20100929 -reg v6v4 &amp;
</pre>
<dl><dd>This also expands the table by adding back all the reads with the same sequence, and adds all the reads that were deleted for quality (thus the reads are never really deleted, they are just flagged.  There is a field in the table for whether the read was deleted: deleted=1 for deleted, deleted=0 for not deleted)
</dd></dl>
<p>6.  Check the results in the gast and gast_concat tables:
</p>
<pre>       gast_check 20100929 v6v4
</pre>
<dl><dd><dl><dd><dl><dd>which performs the following series of database queries:
<dl><dd>select count(distinct read_id) from gast_20100929_v6v4 where refhvr_id=0 and distance = 1          
</dd></dl>
</dd><dd>which is the total number of reads that were not assigned a taxonomy by gast.  This should be a slightly larger number that the number of sequences that were not assigned a taxonomy from (4) above.
</dd><dd>The next four numbers should all be the same. They are the total number of reads in the gast table from (4) above and the number in the main trimseq, gast, and gast_concat tables, found with the following queries:
<dl><dd>select count(read_id) from trimseq where run=20071018 AND deleted=0 and source = 'v6v4';  
</dd><dd>select count(distinct read_id) from gast_20109029_v6v4&nbsp;;
</dd><dd>select count(distinct g.read_id) from gast as g join trimseq as t using(read_id) where run='20100929' and deleted=0 and source = 'v6v4';
</dd><dd>select count(g.read_id) from gast_concat as g join trimseq as t using(read_id) where run='20100929' and deleted=0 and source = 'v6v4';
</dd></dl>
</dd></dl>
</dd><dd>The gast table has a separate entry for each best match to the reference database for each read, thus the "distinct".
</dd><dd>The gast_concat table has a single entry for each read.
</dd></dl>
</dd></dl>
<p>7.  Log back onto the cluster and cd to the gast directory corresponding to the run.  Expand the taxonomic assignment from the sequences to each read with gast2tax, which stores data the tagtax table.
</p>
<dl><dd><dl><dd><dl><dd>Use defaults for the output table, boot scores and majority.
</dd><dd>The reference table should be refhvr_* where * is the vregion from 5' to 3' -- in other words, for v6v4 specify refhvr_v4v6.   
</dd></dl>
</dd></dl>
</dd></dl>
<pre>      clusterize gast2tax -g gast_20100929_v6v4 -reg v6v4 -ref <span style="color:#9400D3">refhvr_v4v6</span>
</pre>
<dl><dd><dl><dd>To verify, count the reads in the tagtax table from your run.  The count should match the sum of the counts from step (6) above.
</dd></dl>
</dd></dl>
<pre>     mysql -h jbpcdb.mbl.edu env454 -e "select count(*) from tagtax as x join trimseq as t using(read_id) where run='20100929' and deleted=0"
</pre>
<dl><dd><dl><dd>You can get some summary information about the taxonomic diversity of your run with this query:
</dd></dl>
</dd></dl>
<pre>     mysql -h jbpcdb.mbl.edu env454 -e "select taxonomy, count(*) from tagtax as x join trimseq as t using(read_id) where run=20100929 group by taxonomy"
</pre>
<p>8.  When the data are processed, please send an email notification to vamps@mbl.edu with the subject line
</p>
<dl><dd><dl><dd><dl><dd><dl><dd>New VAMPS data ready
</dd></dl>
</dd></dl>
</dd><dd>The body of the email should contain:
<dl><dd><dl><dd>1)  the date of the run, 
</dd><dd>2) projects included in the run (from the SeqReq forms) 
</dd><dd>3) and any users requiring access to each project. 
</dd></dl>
</dd></dl>
</dd></dl>
</dd></dl>
<dl><dd><dl><dd><b>Remember, BPC users automatically have access to all projects, other users are provided access to non-public data on a project by project basis.</b>
</dd></dl>
</dd></dl>
<h2> <span class="mw-headline" id="Uploading_sequences_to_VAMPS"> Uploading sequences to VAMPS </span></h2>
<p>After the sequences have been trimmed, and the taxonomy run, the data are ready to be uploaded
to the VAMPS server.  You must have permissions on the VAMPS database to run this.
</p>
<pre> 1.  cd to the $g454/vamps/tovamps directory.
</pre>
<pre> 2. run "./run_vamps_upload"
</pre>
<pre> 3.  Much of the script will run in the background, and generally takes 2 or 3 hours to run.  
       When it is complete, the upload needs to be confirmed prior to replacing the existing VAMPS data.
       Doing a simple row count is generally sufficient -- checking that the new tables have more records than the old.
    
       Check tables: vamps_data_cube, vamps_export, vamps_junk_data_cube, vamps_projects_datasets,
        vamps_sequences and vamps_taxonomy, by getting a record count for each, and comparing this to the table with
       the same name, although having a "_transfer" suffix.
</pre>
<pre> 4.  If all tables have the been properly loaded, they can be swapped in, and the vamps_dev website uploaded as well.
       a. run  "./run_vamps_upload_transfer"
       b. run "./run_vamps_upload_dev"
</pre>
<h2> <span class="mw-headline" id="Clustering_Sequences"> Clustering Sequences </span></h2>
<p>After the sequences have been trimmed, they need to be clustered for estimating OTUs and calculating diversity.
<b>Please do not try this on v6v4 data: check with Sue or David first</b>
</p>
<pre> 1. go to $g454/dotur/vamps
</pre>
<pre> 2. Use vamps_dotur_clusters for all new projects:  
  
     vamps_dotur_clusters - runs DOTUR on all v6 datasets and packages the 
                       the data by project and file type for copying to vamps
                       Clusters v3, v6, and v9 projects only
   
     Usage:  vamps_dotur_clusters [update | all | project]
         ex:  vamps_dotur_clusters update
         ex:  vamps_dotur_clusters project=ICM_DSS_Bv6
   
    Options:  
          update    updates only new (not yet included) projects 
                    (does not have a directory in $g454/dotur/data/tovamps)
   
          all       reruns all projects in the database (v3, v6, v9)
  
 Run:
    vamps_dotur_clusters update
</pre>
<pre> 3:  This creates a series of run_* scripts that should be run in order.
      a. From the cluster run:
     
          run_mkdir_1
          run_rarify_2
          run_tax_3

      b.  When the clusters are complete, log onto a bioinformatics machine

           run_pack_clusters_4
           run_scp2vamps_5
           run_chmod_6
 
      c.  Log onto the vamps server(s) and double-check the permissions in 
            /usr/local/www/vamps/docs/data_downloads/dotur/*
   
          The permissions -rw-rw---- should be set by the script, but in case 
          they are not, do: 
            chmod g+w *
</pre>
<h2> <span class="mw-headline" id="Adding_new_users"> Adding new users </span></h2>
<p>For adding new user accounts, [<a href="https://vampsdev.mbl.edu/documentation/index.php/Adding_New_Users" class="external text" rel="nofollow">| See the VAMPS wiki. </a>]
</p><p>Adding a new project to vamps through the vamps_users table. Do this on vampsdev and vamps 
</p>
<pre>   insert into vamps_users (user, project) values ('username' ,'project')
</pre>
<h2> <span class="mw-headline" id="Taxonomy_Spreadsheets">Taxonomy Spreadsheets</span></h2>
<p>Once the taxonomy has been completed in the database, Excel spreadsheets are used to communicate
the information to the researchers.  This step only needed for v9 data.
</p>
<pre> 1.  Copy the template.xls spreadsheet to a new Excel spreadsheet, named for the project not the run, 
      as researchers often do not know the run dates.
</pre>
<pre> 2.  Go to the SQL tab, and begin editing.  The first thing to find is the relative abundance of tags 
      for each dataset.  Rerun this SQL and paste the results back into the SQL tab.
</pre>
<pre> 3.  Edit the 6 taxonomy SQL statements:
      Update the datasets in the COUNT(CASE statements, 
      Update the normalizing factors in each COUNT(CASE statement, using the largest frequency of 
           all datasets as the numerator, and the frequency for that specific dataset as the denominator.
      Update the run and project in the WHERE statement.
</pre>
<pre> 4.  For each tab, copy the revised SQL statement, open the ODBC connection, select all and delete
       the previous (mangled) SQL and paste in the new.
</pre>
<pre> 5.  For each normalized tab, select all columns that contain sequence counts (but not distance to V6) and 
       reduce the decimal places to 0
</pre>
<pre> 6.  For normalized tabs, recalculate each Total column, then reorder the data using Total descending
</pre>
<pre>  7.  Check totals for Mitch on the TaxByTag-NormVote sheet.  Insert a row above the dataset and 
       calculate total number of tags for each dataset.
</pre>
<pre>  8.  Store a copy of the final spreadsheet(s) in g454/exports/taxonomies.
</pre>
<!-- 
NewPP limit report
Preprocessor node count: 35/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key bpcwiki:pcache:idhash:1434-0!1!0!!en!2!edit=0 and timestamp 20111111142015 -->
<div class="printfooter">
Retrieved from "<a href="https://jbpc.mbl.edu/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline">https://jbpc.mbl.edu/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline</a>"</div>
		<div id='catlinks' class='catlinks catlinks-allhidden'></div>		<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline" title="View the content page [c]" accesskey="c">Page</a></li>
				 <li id="ca-talk" class="new"><a href="/orderforms/wiki/index.php?title=Talk:454_Bioinformatics_Pipeline&amp;action=edit&amp;redlink=1" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-viewsource"><a href="/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e">View source</a></li>
				 <li id="ca-history"><a href="/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline&amp;action=history" title="Past revisions of this page [h]" accesskey="h">History</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/orderforms/wiki/index.php?title=Special:UserLogin&amp;returnto=454_Bioinformatics_Pipeline" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/images/mbl_color_logo_140x60.jpg);" href="/orderforms/wiki/index.php?title=Main_Page" title="Visit the main page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage"><a href="/orderforms/wiki/index.php?title=Main_Page" title="Visit the main page [z]" accesskey="z">Main Page</a></li>
				<li id="n-Protocols"><a href="/orderforms/wiki/index.php?title=Protocols">Protocols</a></li>
				<li id="n-Computing-FAQ"><a href="/orderforms/wiki/index.php?title=Computing_FAQ">Computing FAQ</a></li>
				<li id="n-Bioinformatics-Resources"><a href="/orderforms/wiki/index.php?title=Bioinformatics_Resources">Bioinformatics Resources</a></li>
				<li id="n-Bay-Paul-Community-Resources"><a href="/orderforms/wiki/index.php?title=General_Bay_Paul_Resources">Bay Paul Community Resources</a></li>
				<li id="n-recentchanges"><a href="/orderforms/wiki/index.php?title=Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-Edit-this-nav-bar"><a href="/orderforms/wiki/index.php?title=MediaWiki:Sidebar">Edit this nav-bar</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/orderforms/wiki/index.php" id="searchform">
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" title="Search BayPaulWiki" accesskey="f" type="search" name="search" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search the pages for this text" />
			</form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/orderforms/wiki/index.php?title=Special:WhatLinksHere/454_Bioinformatics_Pipeline" title="List of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/orderforms/wiki/index.php?title=Special:RecentChangesLinked/454_Bioinformatics_Pipeline" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-specialpages"><a href="/orderforms/wiki/index.php?title=Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/orderforms/wiki/index.php?title=454_Bioinformatics_Pipeline&amp;oldid=7378" title="Permanent link to this revision of the page">Permanent link</a></li>			</ul>
		</div>
	</div>
</div><!-- end of the left (by default at least) column -->
<div class="visualClear"></div>
<div id="footer">
	<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/orderforms/wiki/skins/common/images/poweredby_mediawiki_88x31.png" height="31" width="88" alt="Powered by MediaWiki" /></a></div>
	<ul id="f-list">
		<li id="lastmod"> This page was last modified on 9 May 2011, at 13:12.</li>
		<li id="viewcount">This page has been accessed 3,875 times.</li>
		<li id="privacy"><a href="/orderforms/wiki/index.php?title=BayPaulWiki:Privacy_policy" title="BayPaulWiki:Privacy policy">Privacy policy</a></li>
		<li id="about"><a href="/orderforms/wiki/index.php?title=BayPaulWiki:About" title="BayPaulWiki:About">About BayPaulWiki</a></li>
		<li id="disclaimer"><a href="/orderforms/wiki/index.php?title=BayPaulWiki:General_disclaimer" title="BayPaulWiki:General disclaimer">Disclaimers</a></li>
	</ul>
</div>
</div>

<script>if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served in 0.465 secs. --></body></html>
